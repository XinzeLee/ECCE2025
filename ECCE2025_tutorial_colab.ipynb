{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f848a947",
      "metadata": {
        "id": "f848a947"
      },
      "source": [
        "## Section 1. Install Python Dependencies\n",
        "\n",
        "* Following the index from 1 onwards\n",
        "* Recommended method: use `pip` to install dependencies.\n",
        "* If running in Google Colab, you can use a code cell with `!pip install ...` to install missing packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9fb761",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9fb761",
        "outputId": "38bb3fcc-bec9-4083-b536-5fca3b401d83"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/XinzeLee/ECCE2025 /content/ECCE2025\n",
        "# change current working directory\n",
        "%cd /content/ECCE2025\n",
        "\n",
        "# If requirements.txt is already in your working directory\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588fbed2",
      "metadata": {
        "id": "588fbed2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b3f5c569",
      "metadata": {
        "id": "b3f5c569"
      },
      "source": [
        "## Section 2. Load and Explore Your Data\n",
        "    2.1 Load Data: Import your dataset and implement basic data cleaning.\n",
        "\n",
        "    2.2 Feature Selection (PCA): Use Principal Component Analysis to identify key features.\n",
        "\n",
        "    2.3 Exploratory Data Analysis (EDA): Visualize and summarize your data to uncover patterns and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a92ec8b",
      "metadata": {
        "id": "1a92ec8b"
      },
      "source": [
        "#### Section 2.1 Load Data: Import the DAB dataset and implement basic data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TgC6NuApQJYD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "id": "TgC6NuApQJYD",
        "outputId": "75b4c105-440e-4497-c5d5-eb8f02e9703b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
              "   <b style=\"color:#222222;\">Hands-on Experiment 1:</b>\n",
              "   <span style=\"color:#333333;\">Remove outliers and/or filter by total_ZVS</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
        "   <b style=\"color:#222222;\">Hands-on Experiment 1:</b>\n",
        "   <span style=\"color:#333333;\">Remove outliers and/or filter by total_ZVS</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef57e4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eef57e4a",
        "outputId": "03aec06d-0e8b-44dd-8f5c-c1a9988fbb33"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Conduct basic data cleaning\n",
        "# TODO: Hands-on Experiment 1 - Remove outliers and/or filter by total_ZVS\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Flag to control whether to apply outlier exclusion and total_ZVS filtering\n",
        "# Set to False to skip both outlier and total_ZVS filtering\n",
        "# @title  {\"form-width\":\"40%\"}\n",
        "apply_data_filtering = False # @param [\"True\",\"False\"] {\"type\":\"raw\"}\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"DAB_TPS.csv\", index_col=0)\n",
        "\n",
        "# Print initial shape\n",
        "print(\"Initial shape:\", df.shape)\n",
        "\n",
        "# Filter out data where \"Validity\" column is 0\n",
        "df_filtered = df[df['Validity'] != 0]\n",
        "print(\"Shape after Validity filtering:\", df_filtered.shape)\n",
        "\n",
        "# Sum the specified columns\n",
        "sum_columns = [\"vp_t1\", \"vp_t2\", \"vp_t3\", \"vp_t4\", \"vs_t1\", \"vs_t2\", \"vs_t3\", \"vs_t4\"]\n",
        "df_filtered['total_ZVS'] = df_filtered[sum_columns].sum(axis=1)\n",
        "\n",
        "if apply_data_filtering:\n",
        "    # Filter out samples with odd sum values\n",
        "    df_intermediate = df_filtered[df_filtered['total_ZVS'] % 2 == 0]\n",
        "    print(\"Shape after total_ZVS even filtering:\", df_intermediate.shape)\n",
        "\n",
        "    # Analyze outliers in the 'ipk2pk' feature\n",
        "    ipk2pk = df_intermediate['ipk2pk']\n",
        "    mean_ipk2pk = ipk2pk.mean()\n",
        "    std_ipk2pk = ipk2pk.std()\n",
        "\n",
        "    # Define outlier threshold (e.g., 3 standard deviations from mean)\n",
        "    upper_threshold = mean_ipk2pk + 3 * std_ipk2pk\n",
        "    lower_threshold = mean_ipk2pk - 3 * std_ipk2pk\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df_intermediate[(ipk2pk > upper_threshold) | (ipk2pk < lower_threshold)]\n",
        "\n",
        "    print(f\"ipk2pk mean: {mean_ipk2pk:.3f}, std: {std_ipk2pk:.3f}\")\n",
        "    print(f\"Outlier threshold: < {lower_threshold:.3f} or > {upper_threshold:.3f}\")\n",
        "    print(f\"Number of outliers in ipk2pk: {outliers.shape[0]}\")\n",
        "\n",
        "    # Exclude outliers\n",
        "    df_final = df_intermediate[(ipk2pk <= upper_threshold) & (ipk2pk >= lower_threshold)]\n",
        "    print(f\"Shape after outlier exclusion: {df_final.shape}\")\n",
        "else:\n",
        "    print(\"total_ZVS and outlier filtering not applied.\")\n",
        "    df_final = df_filtered.copy()\n",
        "    print(f\"Shape after skipping total_ZVS and outlier filtering: {df_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9cf6eb",
      "metadata": {
        "id": "de9cf6eb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f69b85d7",
      "metadata": {
        "id": "f69b85d7"
      },
      "source": [
        "#### 2.2 Feature Selection (PCA): Use Principal Component Analysis to identify key features\n",
        "* Correlation map: Feature correlation\n",
        "* Histogram: Visualize the distribution of input and output variables\n",
        "* Principal Component Analysis: Guide the selection of main features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8333cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cc8333cc",
        "outputId": "5548c1b9-b707-4443-dc37-b64c8f0d310c"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Conduct basic data analysis, including correlation map, histograms, feature ranges, etc., to guide the selection of main features\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the features we want to analyze\n",
        "input_features = [\"Vref\", \"P\", \"D0\", \"D1\", \"D2\"]\n",
        "output_features = [\"ipk2pk\", \"total_ZVS\"]\n",
        "all_features = input_features + output_features\n",
        "\n",
        "# Import the functions to plot the correlation map, feature histograms, and basic statistical analysis\n",
        "from utils import plot_correlation_map, plot_feature_histograms, basic_statistical_analysis\n",
        "\n",
        "# 1. Correlation Map\n",
        "plot_correlation_map(df_final, all_features)\n",
        "\n",
        "# 2. Histograms for each feature\n",
        "plot_feature_histograms(df_final, all_features)\n",
        "\n",
        "# 3. Basic Statistical Analysis\n",
        "basic_statistical_analysis(df_final, all_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c960cd1",
      "metadata": {
        "id": "2c960cd1"
      },
      "source": [
        "![f6407750-56df-43ab-b046-4bbf8156973a.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7477e8cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7477e8cc",
        "outputId": "2ab3e939-a269-486f-e82e-fe03723e18c2"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Conduct PCA analysis, remember to standardize the features first\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Perform PCA on the selected features\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select the features for PCA\n",
        "pca_features = [\"Vref\", \"P\", \"D1\", \"D2\", \"D0\"]\n",
        "X_pca = df_final[pca_features]\n",
        "\n",
        "# # Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_pca)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "X_pca_transformed = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Import the function to plot and summarize the PCA results\n",
        "from utils import plot_and_summarize_pca_results\n",
        "\n",
        "plot_and_summarize_pca_results(\n",
        "    explained_variance_ratio,\n",
        "    cumulative_variance_ratio,\n",
        "    X_pca_transformed,\n",
        "    pca,\n",
        "    pca_features\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2dcfbd",
      "metadata": {
        "id": "7a2dcfbd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "41bedb1e",
      "metadata": {
        "id": "41bedb1e"
      },
      "source": [
        "#### 2.3 Exploratory Data Analysis (EDA): Visualize and summarize your data to uncover patterns and insights\n",
        "* t-SNE algorithm: Embeds dataset in feature domain and reveals latent feature manifolds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9598a990",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9598a990",
        "outputId": "5c9aa7bc-f6ec-499d-eff9-667872763348"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Conduct t-SNE analysis, including 1) input, 2) output, and 3) combined features\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# t-SNE Visualization for Input, Output, and Combined Features\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Boolean variable to specify whether to load trained t-SNE models from local drive\n",
        "load_trained_tsne = True  # Set to True to load from disk, False to train anew\n",
        "\n",
        "# Prepare data for t-SNE\n",
        "input_data = df_final[input_features].values\n",
        "output_data = df_final[output_features].values\n",
        "combined_data = df_final[input_features + output_features].values\n",
        "\n",
        "# Get unique voltage levels and power levels for coloring\n",
        "voltage_levels = df_final['Vref'].unique()\n",
        "power_levels = df_final['P'].unique()\n",
        "voltage_colors = plt.cm.Set1(np.linspace(0, 1, len(voltage_levels)))\n",
        "power_colors = plt.cm.tab10(np.linspace(0, 1, len(power_levels)))\n",
        "voltage_color_map = dict(zip(voltage_levels, voltage_colors))\n",
        "power_color_map = dict(zip(power_levels, power_colors))\n",
        "\n",
        "if load_trained_tsne and os.path.exists(\"tsne_input_embedding.npy\") and os.path.exists(\"tsne_output_embedding.npy\") and os.path.exists(\"tsne_combined_embedding.npy\"):\n",
        "    print(\"Loading precomputed t-SNE embeddings from local drive...\")\n",
        "    input_tsne = np.load(\"tsne_input_embedding.npy\")\n",
        "    output_tsne = np.load(\"tsne_output_embedding.npy\")\n",
        "    combined_tsne = np.load(\"tsne_combined_embedding.npy\")\n",
        "    print(\"t-SNE features are loaded. Please conduct the next block to visualize the feature domain.\")\n",
        "else:\n",
        "    # 1. t-SNE for Input Features\n",
        "    print(\"Computing t-SNE for input features...\")\n",
        "    tsne_input = TSNE(n_components=2, random_state=42, perplexity=200)\n",
        "    input_tsne = tsne_input.fit_transform(input_data)\n",
        "    # Save the t-SNE embedding and model\n",
        "    np.save(\"tsne_input_embedding.npy\", input_tsne)\n",
        "\n",
        "    # 2. t-SNE for Output Features\n",
        "    print(\"Computing t-SNE for output features...\")\n",
        "    tsne_output = TSNE(n_components=2, random_state=42, perplexity=200)\n",
        "    output_tsne = tsne_output.fit_transform(output_data)\n",
        "    np.save(\"tsne_output_embedding.npy\", output_tsne)\n",
        "\n",
        "    # 3. t-SNE for Combined Features\n",
        "    print(\"Computing t-SNE for combined features...\")\n",
        "    tsne_combined = TSNE(n_components=2, random_state=42, perplexity=200)\n",
        "    combined_tsne = tsne_combined.fit_transform(combined_data)\n",
        "    np.save(\"tsne_combined_embedding.npy\", combined_tsne)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Tfko6SI0Ptls",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "Tfko6SI0Ptls",
        "outputId": "8241abc2-865b-4b3c-f8ea-263e71dea00f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background:#ffe599; padding:12px 14px; border-radius:8px; text-align:center; font-size:1.25rem;\">\n",
              "  <b style=\"color:#222;\">Hands-on Experiment 2:</b>\n",
              "  <span style=\"color:#333;\">Distinguish power levels or not</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background:#ffe599; padding:12px 14px; border-radius:8px; text-align:center; font-size:1.25rem;\">\n",
        "  <b style=\"color:#222;\">Hands-on Experiment 2:</b>\n",
        "  <span style=\"color:#333;\">Distinguish power levels or not</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445f2439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "445f2439",
        "outputId": "01e460ff-dcd3-44c5-a2ee-f3e277379981"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Visualize t-SNE results\n",
        "# TODO: Hands-on Experiment 2 - Distinguish power levels or not\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# import the function to plot the t-SNE analysis\n",
        "from utils import plot_tsne_analysis\n",
        "\n",
        "# Flag to control whether to plot power levels\n",
        "# Set to False to only plot voltage levels\n",
        "# @title  {\"form-width\":\"40%\"}\n",
        "plot_power_levels = False # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "\n",
        "\n",
        "print(\"Plotting t-SNE analysis...\")\n",
        "kwargs = {}\n",
        "if plot_power_levels:\n",
        "    kwargs = dict(power_levels=power_levels, power_color_map=power_color_map)\n",
        "plot_tsne_analysis(\n",
        "    df_final,\n",
        "    voltage_levels,\n",
        "    voltage_color_map,\n",
        "    input_tsne,\n",
        "    output_tsne,\n",
        "    combined_tsne,\n",
        "    input_features,\n",
        "    output_features,\n",
        "    **kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad35e989",
      "metadata": {
        "id": "ad35e989"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "56c78981",
      "metadata": {
        "id": "56c78981"
      },
      "source": [
        "## Section 3. Data-Driven Modeling of Current Stress and ZVS\n",
        "    3.1 Feedforward NN for Current Stress *Regression*\n",
        "\n",
        "    3.2 Feedforward NN for ZVS *Classification*\n",
        "\n",
        "    3.3 Anytime Algorithms: Ensemble Learning Algorithms - Random Forest and XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39d239fe",
      "metadata": {
        "id": "39d239fe"
      },
      "source": [
        "#### 3.1 Feedforward NN for Current Stress **Regression**\n",
        "    Conduct the following experiments one-by-one to understand its effects\n",
        "* Standardize inputs: Scale inputs within [0, 1]; or normalize to normal distribution with Norm(0, 1)\n",
        "* Normalization layer: Normalize fully-connected layer's output to accelerate training and convergence\n",
        "* Z-score weights: Similar effects as normalization layer\n",
        "* Learning rate scheduler: Update learning rate heuristically\n",
        "* Tune some NN hyper-parameters: 1) Hidden size; 2) Number of layers\n",
        "* Impacts of activation functions on function surface (regression): 1) ReLU; 2) tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nr4mMu6fQViv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "id": "nr4mMu6fQViv",
        "outputId": "508e9ead-4bdc-4d36-aec9-57219c7434d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
              "   <b style=\"color:#222222;\">Hands-on Experiment 3:</b>\n",
              "   <span style=\"color:#333333;\">Develop FNN for Current Stress Modeling</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
        "   <b style=\"color:#222222;\">Hands-on Experiment 3:</b>\n",
        "   <span style=\"color:#333333;\">Develop FNN for Current Stress Modeling</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad91f06a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad91f06a",
        "outputId": "1bd5d436-8008-4f1d-adac-a2106ea5e170"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Define the neural network architecture, implement the training process, and tune the hyper-parameters\n",
        "# TODO: Hands-on Experiment 3 - Develop FNN for Current Stress Modeling\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# =========================\n",
        "# Configurable flags/params\n",
        "# =========================\n",
        "# @title  {\"form-width\":\"40%\"}\n",
        "# Flag: whether to standardize input features\n",
        "USE_STANDARDIZE = False # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to use batch normalization\n",
        "USE_BATCHNORM = False # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to z-score gradients\n",
        "USE_ZSCORE_GRAD = False # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to use learning rate scheduler\n",
        "USE_LR_SCHEDULER = False # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "\n",
        "# Parameter: hidden layer size\n",
        "HIDDEN_SIZE = 32 # @param {\"type\":\"slider\",\"min\":4,\"max\":128,\"step\":4}\n",
        "# Parameter: number of hidden layers\n",
        "NUM_HIDDEN_LAYERS = 1 # @param {\"type\":\"slider\",\"min\":1,\"max\":3,\"step\":1}\n",
        "\n",
        "# Variable: activation function, options: \"relu\", \"tanh\"\n",
        "ACTIVATION = \"relu\" # @param [\"relu\",\"tanh\"]\n",
        "\n",
        "# =========================\n",
        "# Activation function selector\n",
        "# =========================\n",
        "def get_activation(name):\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    elif name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation: {name}\")\n",
        "\n",
        "# =========================\n",
        "# Neural network definition\n",
        "# =========================\n",
        "class CurrentStressMLP(nn.Module):\n",
        "    def __init__(self, input_size=4, hidden_size=32, num_hidden_layers=2, activation=\"relu\", use_batchnorm=True):\n",
        "        super(CurrentStressMLP, self).__init__()\n",
        "        layers = []\n",
        "        in_dim = input_size\n",
        "        act = get_activation(activation)\n",
        "        for i in range(num_hidden_layers):\n",
        "            layers.append(nn.Linear(in_dim, hidden_size))\n",
        "            if use_batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(act)\n",
        "            in_dim = hidden_size\n",
        "        layers.append(nn.Linear(hidden_size, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# =========================\n",
        "# Custom dataset class\n",
        "# =========================\n",
        "class CurrentStressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================\n",
        "# Prepare the data\n",
        "# =========================\n",
        "input_features = [\"P\", \"Vref\", \"D1\", \"D2\"]\n",
        "output_feature = \"ipk2pk\"\n",
        "\n",
        "# Extract features and target\n",
        "X = df_final[input_features].values\n",
        "y = df_final[output_feature].values\n",
        "\n",
        "# Split the data: train/val/test (60/20/20)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Feature scaling or normalization is important for the training of the neural network\n",
        "if USE_STANDARDIZE:\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "else:\n",
        "    X_train_scaled = X_train\n",
        "    X_val_scaled = X_val\n",
        "    X_test_scaled = X_test\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CurrentStressDataset(X_train_scaled, y_train)\n",
        "val_dataset = CurrentStressDataset(X_val_scaled, y_val)\n",
        "test_dataset = CurrentStressDataset(X_test_scaled, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# =========================\n",
        "# Initialize the model\n",
        "# =========================\n",
        "model_NN_regression = CurrentStressMLP(\n",
        "    input_size=len(input_features),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "    activation=ACTIVATION,\n",
        "    use_batchnorm=USE_BATCHNORM\n",
        ")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_NN_regression.parameters(), lr=0.1)  # Higher initial learning rate\n",
        "\n",
        "if USE_LR_SCHEDULER:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)  # Reduce LR every 200 epochs\n",
        "\n",
        "# =========================\n",
        "# Training loop\n",
        "# =========================\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []  # <--- Add test loss tracking\n",
        "best_val_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Training the neural network...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model_NN_regression.train()\n",
        "    train_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_NN_regression(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient z-scoring if enabled\n",
        "        if USE_ZSCORE_GRAD:\n",
        "            for param in model_NN_regression.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_mean = param.grad.mean()\n",
        "                    grad_std = param.grad.std()\n",
        "                    if grad_std > 0:\n",
        "                        param.grad = (param.grad - grad_mean) / grad_std\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Update learning rate if enabled\n",
        "    if USE_LR_SCHEDULER:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model_NN_regression.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model_NN_regression(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Test phase (record test loss)\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model_NN_regression(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    test_losses.append(test_loss / len(test_loader))\n",
        "\n",
        "    # Save best model on validation set\n",
        "    if val_losses[-1] < best_val_loss:\n",
        "        best_val_loss = val_losses[-1]\n",
        "        best_model_state = copy.deepcopy(model_NN_regression.state_dict())\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        if USE_LR_SCHEDULER:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "        else:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], LR: {current_lr:.6f}, Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}, Test Loss: {test_losses[-1]:.6f}')\n",
        "\n",
        "# Load best model weights (on validation set)\n",
        "if best_model_state is not None:\n",
        "    model_NN_regression.load_state_dict(best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620da914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "620da914",
        "outputId": "933d2b0c-ba35-4b30-ec80-5c37c9b2a835"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Plot and report the model performance\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Evaluate the model\n",
        "model_NN_regression.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model_NN_regression(torch.FloatTensor(X_test_scaled)).numpy().flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "from utils import plot_and_report_model_performance\n",
        "\n",
        "# Plot and report the model performance\n",
        "plot_and_report_model_performance(\n",
        "    mse, rmse, r2,\n",
        "    train_losses, test_losses,\n",
        "    y_test, y_pred,\n",
        "    model_NN_regression, X_test_scaled, input_features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887fa4e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "887fa4e2",
        "outputId": "a5a47837-407f-42c9-b3a9-bb4a113ddadf"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Observe the model surface and contour plots, change the activation function and understand the effects\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 3D Mesh plot of current stress with respect to D1 and D2\n",
        "print(f\"\\n=== 3D MESH PLOT ===\")\n",
        "print(\"Generating 3D mesh plot of current stress vs D1 and D2 (P=100, Vref=240)...\")\n",
        "\n",
        "# Create meshgrid for D1 and D2\n",
        "D1_range = np.linspace(X_test_scaled[:, 2].min(), X_test_scaled[:, 2].max(), 50)  # D1 is at index 2\n",
        "D2_range = np.linspace(X_test_scaled[:, 3].min(), X_test_scaled[:, 3].max(), 50)  # D2 is at index 3\n",
        "D1_mesh, D2_mesh = np.meshgrid(D1_range, D2_range)\n",
        "\n",
        "# Fixed values for P and Vref (scaled)\n",
        "P_fixed = 100\n",
        "Vref_fixed = 160\n",
        "\n",
        "# Transform the fixed values using the same scaler\n",
        "P_scaled = scaler.transform([[P_fixed, 0, 0, 0]])[0, 0]  # Get P value\n",
        "Vref_scaled = scaler.transform([[0, Vref_fixed, 0, 0]])[0, 1]  # Get Vref value\n",
        "\n",
        "# Create a 3D mesh plot of the current stress vs D1 and D2\n",
        "# Create input array for prediction\n",
        "X_contour = np.zeros((D1_mesh.size, 4))\n",
        "X_contour[:, 0] = P_scaled  # P (fixed) - index 0\n",
        "X_contour[:, 1] = Vref_scaled  # Vref (fixed) - index 1\n",
        "X_contour[:, 2] = D1_mesh.flatten()  # D1 - index 2\n",
        "X_contour[:, 3] = D2_mesh.flatten()  # D2 - index 3\n",
        "\n",
        "# Predict current stress\n",
        "model_NN_regression.eval()\n",
        "with torch.no_grad():\n",
        "    stress_pred = model_NN_regression(torch.FloatTensor(X_contour)).numpy().flatten()\n",
        "\n",
        "# Reshape predictions back to meshgrid shape\n",
        "stress_mesh = stress_pred.reshape(D1_mesh.shape)\n",
        "\n",
        "# Inverse transform D1 and D2 back to original scale\n",
        "D1_original = np.zeros((D1_mesh.size, 4))\n",
        "D1_original[:, 2] = D1_mesh.flatten()  # Set D1 values\n",
        "D1_original = scaler.inverse_transform(D1_original)\n",
        "D1_original = D1_original[:, 2].reshape(D1_mesh.shape)\n",
        "\n",
        "\n",
        "D2_original = np.zeros((D2_mesh.size, 4))\n",
        "D2_original[:, 3] = D2_mesh.flatten()  # Set D2 values\n",
        "D2_original = scaler.inverse_transform(D2_original)\n",
        "D2_original = D2_original[:, 3].reshape(D2_mesh.shape)\n",
        "\n",
        "# Plot the 3D mesh and 2D contour plot of the current stress\n",
        "from utils import plot_stress_mesh_and_contour\n",
        "\n",
        "plot_stress_mesh_and_contour(D1_original, D2_original, stress_mesh,\n",
        "                                 mesh_title=f'3D Mesh Plot: Current Stress vs D1 and D2\\n(P={P_fixed}, Vref={Vref_fixed})',\n",
        "                                 contour_title=f'2D Contour Plot: Current Stress vs D1 and D2\\n(P={P_fixed}, Vref={Vref_fixed})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86749b6a",
      "metadata": {
        "id": "86749b6a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4338a0a4",
      "metadata": {
        "id": "4338a0a4"
      },
      "source": [
        "#### 3.2 Feedforward NN for Zero Voltage Switching **Classification**\n",
        "    Conduct the following experiments one-by-one to understand its effects\n",
        "* Standardize inputs: Scale inputs within [0, 1]; or normalize to normal distribution with Norm(0, 1)\n",
        "* Normalization layer: Normalize fully-connected layer's output to accelerate training and convergence\n",
        "* Z-score weights: Similar effects as normalization layer\n",
        "* Learning rate scheduler: Update learning rate heuristically\n",
        "* Tune some NN hyper-parameters: 1) Hidden size; 2) Number of layers\n",
        "* Impacts of activation functions on decision plane (classification): 1) ReLU; 2) tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eJBTfDEZQa-T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "id": "eJBTfDEZQa-T",
        "outputId": "3c1eba85-d7d5-4b35-a4a7-7062229e5bb9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
              "   <b style=\"color:#222222;\">Hands-on Experiment 4:</b>\n",
              "   <span style=\"color:#333333;\">Develop FNN for Zero Voltage Switching Modeling</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
        "   <b style=\"color:#222222;\">Hands-on Experiment 4:</b>\n",
        "   <span style=\"color:#333333;\">Develop FNN for Zero Voltage Switching Modeling</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea4a1ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cea4a1ac",
        "outputId": "0e9d1f3a-4ead-4f5d-8900-3c03b079853a"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Define the neural network architecture, implement the training process, and tune the hyper-parameters\n",
        "# TODO: Hands-on Experiment 4 - Develop FNN for Zero Voltage Switching Modeling\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# =========================\n",
        "# Configurable flags/params\n",
        "# =========================\n",
        "# @title  {\"form-width\":\"40%\"}\n",
        "# Flag: whether to standardize input features\n",
        "USE_STANDARDIZE = True # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to use batch normalization\n",
        "USE_BATCHNORM = True # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to z-score gradients\n",
        "USE_ZSCORE_GRAD = True # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "# Flag: whether to use learning rate scheduler\n",
        "USE_LR_SCHEDULER = True # @param [\"False\",\"True\"] {\"type\":\"raw\"}\n",
        "\n",
        "# Parameter: hidden layer size\n",
        "HIDDEN_SIZE = 32 # @param {\"type\":\"slider\",\"min\":4,\"max\":128,\"step\":4}\n",
        "# Parameter: number of hidden layers\n",
        "NUM_HIDDEN_LAYERS = 2 # @param {\"type\":\"slider\",\"min\":1,\"max\":3,\"step\":1}\n",
        "\n",
        "# Variable: activation function, options: \"relu\", \"tanh\"\n",
        "ACTIVATION = \"relu\" # @param [\"relu\",\"tanh\"]\n",
        "\n",
        "# =========================\n",
        "# Activation function selector\n",
        "# =========================\n",
        "def get_activation(name):\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    elif name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation: {name}\")\n",
        "\n",
        "# =========================\n",
        "# Neural network definition for ZVS classification\n",
        "# =========================\n",
        "class ZVSClassificationMLP(nn.Module):\n",
        "    def __init__(self, input_size=4, hidden_size=32, num_hidden_layers=2, activation=\"relu\", use_batchnorm=True, num_classes=3):\n",
        "        super(ZVSClassificationMLP, self).__init__()\n",
        "        layers = []\n",
        "        in_dim = input_size\n",
        "        act = get_activation(activation)\n",
        "        for i in range(num_hidden_layers):\n",
        "            layers.append(nn.Linear(in_dim, hidden_size))\n",
        "            if use_batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(act)\n",
        "            in_dim = hidden_size\n",
        "        layers.append(nn.Linear(hidden_size, num_classes))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# =========================\n",
        "# Custom dataset class for ZVS classification\n",
        "# =========================\n",
        "class ZVSClassificationDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "        # Map 4->0, 6->1, 8->2 for 3 classes (0,1,2)\n",
        "        self.map_y = {4: 0, 6: 1, 8: 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y_value = self.y[idx].item()\n",
        "        return self.X[idx], self.map_y[y_value]\n",
        "\n",
        "# =========================\n",
        "# Prepare the data\n",
        "# =========================\n",
        "input_features = [\"P\", \"Vref\", \"D1\", \"D2\"]\n",
        "output_feature = \"total_ZVS\"\n",
        "\n",
        "# Extract features and target\n",
        "X = df_final[input_features].values\n",
        "y = df_final[output_feature].values\n",
        "\n",
        "# Split the data: train/val/test (60/20/20)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Feature scaling or normalization is important for the training of the neural network\n",
        "if USE_STANDARDIZE:\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "else:\n",
        "    X_train_scaled = X_train\n",
        "    X_val_scaled = X_val\n",
        "    X_test_scaled = X_test\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = ZVSClassificationDataset(X_train_scaled, y_train)\n",
        "val_dataset = ZVSClassificationDataset(X_val_scaled, y_val)\n",
        "test_dataset = ZVSClassificationDataset(X_test_scaled, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# =========================\n",
        "# Initialize the model\n",
        "# =========================\n",
        "num_classes = len(np.unique([4, 6, 8]))  # Always 3 for this problem\n",
        "model_NN_classification = ZVSClassificationMLP(\n",
        "    input_size=len(input_features),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "    activation=ACTIVATION,\n",
        "    use_batchnorm=USE_BATCHNORM,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_NN_classification.parameters(), lr=0.1)  # Higher initial learning rate\n",
        "\n",
        "if USE_LR_SCHEDULER:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)  # Reduce LR every 200 epochs\n",
        "\n",
        "# =========================\n",
        "# Training loop\n",
        "# =========================\n",
        "num_epochs = 300\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "best_val_loss = float('inf')\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Training the ZVS classification neural network...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model_NN_classification.train()\n",
        "    train_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_NN_classification(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient z-scoring if enabled\n",
        "        if USE_ZSCORE_GRAD:\n",
        "            for param in model_NN_classification.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_mean = param.grad.mean()\n",
        "                    grad_std = param.grad.std()\n",
        "                    if grad_std > 0:\n",
        "                        param.grad = (param.grad - grad_mean) / grad_std\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Update learning rate if enabled\n",
        "    if USE_LR_SCHEDULER:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model_NN_classification.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model_NN_classification(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Test phase (track test loss)\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model_NN_classification(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "    val_losses.append(val_loss / len(val_loader))\n",
        "    test_losses.append(test_loss / len(test_loader))\n",
        "\n",
        "    # Save best model on validation set\n",
        "    if val_losses[-1] < best_val_loss:\n",
        "        best_val_loss = val_losses[-1]\n",
        "        best_model_state = copy.deepcopy(model_NN_classification.state_dict())\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        if USE_LR_SCHEDULER:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "        else:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], LR: {current_lr:.6f}, Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}, Test Loss: {test_losses[-1]:.6f}')\n",
        "\n",
        "# Load best model weights (on validation set)\n",
        "if best_model_state is not None:\n",
        "    model_NN_classification.load_state_dict(best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32c2ee3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d32c2ee3",
        "outputId": "1716bda7-6a50-432d-8705-d921a11531c9"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Plot and report the ZVS classification model performance\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Import required metrics from sklearn\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Evaluate the ZVS classification model\n",
        "model_NN_classification.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = model_NN_classification(torch.FloatTensor(X_test_scaled))\n",
        "    y_pred_classes = torch.argmax(y_pred_probs, dim=1).numpy()\n",
        "\n",
        "# Apply the same mapping to y_test as used in the dataset\n",
        "map_y = {4: 0, 6: 1, 8: 2}  # Map 4->0, 6->1, 8->2 for 3 classes (0,1,2)\n",
        "y_test_mapped = np.array([map_y[y] for y in y_test])\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy = accuracy_score(y_test_mapped, y_pred_classes)\n",
        "precision = precision_score(y_test_mapped, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test_mapped, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test_mapped, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"\\n=== ZVS CLASSIFICATION MODEL PERFORMANCE ===\")\n",
        "print(f\"Accuracy: {accuracy:.6f}\")\n",
        "print(f\"Precision: {precision:.6f}\")\n",
        "print(f\"Recall: {recall:.6f}\")\n",
        "print(f\"F1-Score: {f1:.6f}\")\n",
        "\n",
        "# Plot the classification results\n",
        "from utils import plot_classification_results\n",
        "\n",
        "plot_classification_results(\n",
        "    train_losses,\n",
        "    test_losses,\n",
        "    y_test_mapped,\n",
        "    y_pred_classes,\n",
        "    model_NN_classification,\n",
        "    X_test_scaled,\n",
        "    input_features\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa63297a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "aa63297a",
        "outputId": "38e649ff-38ae-49b4-9609-e8aa3da6befa"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Observe the decision plane, change the activation function and understand the effects\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "print(f\"\\n=== 2D ZVS CLASSIFICATION PLOT ===\")\n",
        "print(\"Generating 2D plot of ZVS classification vs D1 and D2 (P=100, Vref=240)...\")\n",
        "\n",
        "# Create a 2D plot of ZVS classification with respect to the meshgrid of D1 and D2\n",
        "# Create meshgrid for D1 and D2\n",
        "D1_range = np.linspace(X_test_scaled[:, 2].min(), X_test_scaled[:, 2].max(), 500)  # D1 is at index 2\n",
        "D2_range = np.linspace(X_test_scaled[:, 3].min(), X_test_scaled[:, 3].max(), 500)  # D2 is at index 3\n",
        "D1_mesh, D2_mesh = np.meshgrid(D1_range, D2_range)\n",
        "\n",
        "# Fixed values for P and Vref (scaled)\n",
        "P_fixed = 300\n",
        "Vref_fixed = 160\n",
        "\n",
        "# Transform the fixed values using the same scaler\n",
        "P_scaled = scaler.transform([[P_fixed, 0, 0, 0]])[0, 0]  # Get P value\n",
        "Vref_scaled = scaler.transform([[0, Vref_fixed, 0, 0]])[0, 1]  # Get Vref value\n",
        "\n",
        "# Create input array for prediction\n",
        "X_contour = np.zeros((D1_mesh.size, 4))\n",
        "X_contour[:, 0] = P_scaled  # P (fixed) - index 0\n",
        "X_contour[:, 1] = Vref_scaled  # Vref (fixed) - index 1\n",
        "X_contour[:, 2] = D1_mesh.flatten()  # D1 - index 2\n",
        "X_contour[:, 3] = D2_mesh.flatten()  # D2 - index 3\n",
        "\n",
        "# Predict ZVS classification\n",
        "model_NN_classification.eval()\n",
        "with torch.no_grad():\n",
        "    zvs_pred = torch.argmax(model_NN_classification(torch.FloatTensor(X_contour)), dim=1).numpy()\n",
        "\n",
        "# Reshape predictions back to meshgrid shape\n",
        "zvs_mesh = zvs_pred.reshape(D1_mesh.shape)\n",
        "\n",
        "# Inverse transform D1 and D2 back to original scale\n",
        "D1_original = np.zeros((D1_mesh.size, 4))\n",
        "D1_original[:, 2] = D1_mesh.flatten()  # Set D1 values\n",
        "D1_original = scaler.inverse_transform(D1_original)\n",
        "D1_original = D1_original[:, 2].reshape(D1_mesh.shape)\n",
        "\n",
        "D2_original = np.zeros((D2_mesh.size, 4))\n",
        "D2_original[:, 3] = D2_mesh.flatten()  # Set D2 values\n",
        "D2_original = scaler.inverse_transform(D2_original)\n",
        "D2_original = D2_original[:, 3].reshape(D2_mesh.shape)\n",
        "\n",
        "# Plot the 2D ZVS classification plot\n",
        "from utils import plot_2d_zvs_classification\n",
        "\n",
        "# plot the 2D ZVS classification plot\n",
        "plot_2d_zvs_classification(D1_original, D2_original, zvs_mesh, P_fixed, Vref_fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a8e9edd",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddcd828b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
              "   <b style=\"color:#222222;\">Additional Hands-on Experiment:</b>\n",
              "   <span style=\"color:#333333;\">Effects of Regularization for Neural Networks</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
        "   <b style=\"color:#222222;\">Additional Hands-on Experiment:</b>\n",
        "   <span style=\"color:#333333;\">Effects of Regularization for Neural Networks</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd1026d",
      "metadata": {},
      "source": [
        "<a href=\"https://playground.tensorflow.org/\" target=\"_blank\" style=\"font-size:1.15em; color:#1595c0; font-weight:bold; text-decoration:none;\">\n",
        "  &#9654; Neural Network Playground by Tensorflow\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64244a6d",
      "metadata": {
        "id": "64244a6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "14e2701d",
      "metadata": {
        "id": "14e2701d"
      },
      "source": [
        "#### 3.3 **Anytime** Algorithms: Ensemble Learning Algorithms - Random Forest and XGBoost\n",
        "    Implement XGBoost algorithms to model current stress and ZVS performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b569e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f9b569e9",
        "outputId": "64d5c79f-10fd-4bcd-9eb0-80871c92595a"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Define the xgboost model for current stress modeling, and implement the training process\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare the data\n",
        "input_features = [\"P\", \"Vref\", \"D1\", \"D2\"]\n",
        "output_feature = \"ipk2pk\"\n",
        "\n",
        "# Extract features and target\n",
        "X = df_final[input_features].values\n",
        "y = df_final[output_feature].values\n",
        "\n",
        "# Split the data: train/val/test (60/20/20) similar to the reference codes\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=6,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='rmse'\n",
        ")\n",
        "\n",
        "# Training the XGBoost model\n",
        "print(\"Training the XGBoost model...\")\n",
        "xgb_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    eval_set=[(X_val_scaled, y_val)],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = xgb_model.predict(X_train_scaled)\n",
        "y_val_pred = xgb_model.predict(X_val_scaled)\n",
        "y_test_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = r2_score(y_val, y_val_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nXGBoost Results:\")\n",
        "print(f\"Train MSE: {train_mse:.6f}\")\n",
        "print(f\"Val MSE: {val_mse:.6f}\")\n",
        "print(f\"Test MSE: {test_mse:.6f}\")\n",
        "print(f\"Train R: {train_r2:.6f}\")\n",
        "print(f\"Val R: {val_r2:.6f}\")\n",
        "print(f\"Test R: {test_r2:.6f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "feature_names = input_features\n",
        "\n",
        "# Plot the feature importance and predictions\n",
        "from utils import plot_xgboost_feature_importance_and_predictions\n",
        "\n",
        "plot_xgboost_feature_importance_and_predictions(\n",
        "    feature_names=feature_names,\n",
        "    feature_importance=feature_importance,\n",
        "    y_train=y_train,\n",
        "    y_train_pred=y_train_pred,\n",
        "    train_r2=train_r2,\n",
        "    y_test=y_test,\n",
        "    y_test_pred=y_test_pred,\n",
        "    test_r2=test_r2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c536563",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2c536563",
        "outputId": "3b68454d-f80f-4fd8-cde1-27aab70ab01b"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Define the xgboost model for ZVS classification, and implement the training process\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Prepare the data for ZVS classification\n",
        "input_features = [\"P\", \"Vref\", \"D1\", \"D2\"]\n",
        "output_feature = \"total_ZVS\"\n",
        "\n",
        "# Extract features and target\n",
        "X = df_final[input_features].values\n",
        "y = df_final[output_feature].values\n",
        "\n",
        "# Create mapping for ZVS classes\n",
        "map_y = {4: 0, 6: 1, 8: 2}  # Map 4->0, 6->1, 8->2 for 3 classes (0,1,2)\n",
        "y_mapped = np.array([map_y[y_val] for y_val in y])\n",
        "\n",
        "# Split the data: train/val/test (60/20/20) similar to the regression code above\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y_mapped, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train XGBoost classifier with optimized hyperparameters for class imbalance\n",
        "xgb_classifier = xgb.XGBClassifier(\n",
        "    n_estimators=100,          # Number of boosting rounds\n",
        "    max_depth=6,                # Maximum depth of trees\n",
        "    learning_rate=0.2,        # Learning rate (eta)\n",
        "    random_state=42,\n",
        "    eval_metric='merror',        # Multiclass classification error rate\n",
        "    early_stopping_rounds=5,     # Early stopping to prevent overfitting\n",
        ")\n",
        "\n",
        "print(\"Training the ZVS classification XGBoost model...\")\n",
        "xgb_classifier.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    eval_set=[(X_val_scaled, y_val)],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = xgb_classifier.predict(X_train_scaled)\n",
        "y_val_pred = xgb_classifier.predict(X_val_scaled)\n",
        "y_test_pred = xgb_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\n=== XGBOOST ZVS CLASSIFICATION RESULTS ===\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.6f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.6f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.6f}\")\n",
        "\n",
        "# Plot the classification results\n",
        "from utils import classification_analysis\n",
        "\n",
        "classification_analysis(\n",
        "    y_train, y_train_pred, train_accuracy,\n",
        "    y_test, y_test_pred, test_accuracy,\n",
        "    xgb_classifier, input_features,\n",
        "    y_mapped\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d67f9ea",
      "metadata": {
        "id": "2d67f9ea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d0ecf791",
      "metadata": {
        "id": "d0ecf791"
      },
      "source": [
        "## Section 4. Modulation Optimization with Meta-Heuristic Algorithms\n",
        "    Tune the hyperparameters of PSO algorithms and observe searching behaviors\n",
        "* Particle swarm optimization to search within D1 and D2 ranges given P and Vref values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "w4vvSPBsQk1X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "id": "w4vvSPBsQk1X",
        "outputId": "7522eb4a-d29b-4ec1-fea6-a9227659befb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
              "   <b style=\"color:#222222;\">Hands-on Experiment 5:</b>\n",
              "   <span style=\"color:#333333;\">Tune PSO for Modulation Optimization</span>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"background-color:#ffe599; padding:10px; border-radius:5px; text-align:center; font-size:1.35em;\">\n",
        "   <b style=\"color:#222222;\">Hands-on Experiment 5:</b>\n",
        "   <span style=\"color:#333333;\">Tune PSO for Modulation Optimization</span>\n",
        "</div>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8acbe531",
      "metadata": {
        "id": "8acbe531"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------------\n",
        "# Implement the PSO algorithm to search within D1 and D2 ranges given P and Vref values\n",
        "# TODO: Hands-on Experiment 5 - Tune PSO for Modulation Optimization\n",
        "#------------------------------------------------------------------------------------------------\n",
        "\n",
        "import pyswarms as ps\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Set fixed parameters\n",
        "P_fixed = 100\n",
        "Vref_fixed = 160\n",
        "\n",
        "# Parameterize w_start, w_end, and velocity bounds as percentage of search range\n",
        "# @title  {\"form-width\":\"40%\"}\n",
        "# Parameter: Initial value of inertia weight\n",
        "w_start = 0.9 # @param {\"type\":\"slider\",\"min\":0.01,\"max\":0.99,\"step\":0.01}\n",
        "# Parameter: Final value of inertia weight\n",
        "w_end = 0.4 # @param {\"type\":\"slider\",\"min\":0.01,\"max\":0.99,\"step\":0.01}\n",
        "# Set velocity clamp as a percentage of the search range (e.g., 20%)\n",
        "velocity_clamp_percent = 0.2 # @param {\"type\":\"slider\",\"min\":0.01,\"max\":1,\"step\":0.01}\n",
        "\n",
        "# Define D1 and D2 bounds\n",
        "d1_min, d1_max = 0.6, 1.0\n",
        "d2_min, d2_max = 0.6, 1.0\n",
        "\n",
        "d1_range = d1_max - d1_min\n",
        "d2_range = d2_max - d2_min\n",
        "\n",
        "velocity_clamp = (\n",
        "    velocity_clamp_percent * np.array([d1_range, d2_range]) * -1,  # min velocity (negative)\n",
        "    velocity_clamp_percent * np.array([d1_range, d2_range])        # max velocity (positive)\n",
        ")\n",
        "\n",
        "# Define bounds for PSO as required by pyswarms: (lower_bounds, upper_bounds)\n",
        "bounds = (np.array([d1_min, d2_min]), np.array([d1_max, d2_max]))\n",
        "\n",
        "# PSO options: c1 and c2 to 2.05, w will be changed from 0.9 to 0.0 during operation\n",
        "options = {'c1': 0.3, 'c2': 0.7, 'w': 0.9}\n",
        "\n",
        "n_particles = 10\n",
        "dimensions = 2\n",
        "n_iters = 200\n",
        "\n",
        "# Latin Hypercube Initialization for Swarm Positions\n",
        "def latin_hypercube_sampling(n_samples, bounds, random_state=None):\n",
        "    lower, upper = np.array(bounds[0]), np.array(bounds[1])\n",
        "    dim = len(lower)\n",
        "    cut = np.linspace(0, 1, n_samples + 1)\n",
        "    if random_state is not None:\n",
        "        rng = np.random.RandomState(random_state)\n",
        "        u = rng.rand(n_samples, dim)\n",
        "    else:\n",
        "        u = np.random.rand(n_samples, dim)\n",
        "    a = cut[:n_samples]\n",
        "    b = cut[1:n_samples+1]\n",
        "    rdpoints = np.zeros_like(u)\n",
        "    for j in range(dim):\n",
        "        rdpoints[:, j] = u[:, j] * (b - a) + a\n",
        "        if random_state is not None:\n",
        "            rng.shuffle(rdpoints[:, j])\n",
        "        else:\n",
        "            np.random.shuffle(rdpoints[:, j])\n",
        "    samples = lower + rdpoints * (upper - lower)\n",
        "    return samples\n",
        "\n",
        "init_pos = latin_hypercube_sampling(\n",
        "    n_particles,\n",
        "    ([d1_min, d2_min], [d1_max, d2_max]),\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Objective function: minimize current stress, penalize if ZVS != 8\n",
        "def objective(X):\n",
        "    n_particles = X.shape[0]\n",
        "    costs = np.zeros(n_particles)\n",
        "    for i in range(n_particles):\n",
        "        D1, D2 = X[i, 0], X[i, 1]\n",
        "        X_input = np.array([[P_fixed, Vref_fixed, D1, D2]])\n",
        "        X_input_scaled = scaler.transform(X_input)\n",
        "        X_input_tensor = torch.FloatTensor(X_input_scaled)\n",
        "        model_NN_regression.eval()\n",
        "        model_NN_classification.eval()\n",
        "        with torch.no_grad():\n",
        "            current_stress = model_NN_regression(X_input_tensor).squeeze().item()\n",
        "            zvs_output = model_NN_classification(X_input_tensor)\n",
        "            zvs_prediction = torch.argmax(zvs_output, dim=1).item()\n",
        "            zvs_mapping = {0: 4, 1: 6, 2: 8}\n",
        "            zvs_prediction = zvs_mapping[zvs_prediction]\n",
        "        penalty = abs(zvs_prediction - 8) * 1000\n",
        "        costs[i] = current_stress + penalty\n",
        "    return costs\n",
        "\n",
        "optimizer = ps.single.GlobalBestPSO(\n",
        "    n_particles=n_particles,\n",
        "    dimensions=dimensions,\n",
        "    options=options,\n",
        "    bounds=bounds,\n",
        "    velocity_clamp=velocity_clamp,\n",
        "    init_pos=init_pos\n",
        ")\n",
        "\n",
        "cost_history = []\n",
        "pos_history = []\n",
        "swarm_history = []\n",
        "velocity_history = []\n",
        "\n",
        "for i in range(n_iters):\n",
        "    # Linearly interpolate w\n",
        "    if i <= n_iters-50:\n",
        "        w = w_start + (w_end - w_start) * (i / ((n_iters-50) - 1))\n",
        "    else:\n",
        "        w = w_end\n",
        "    optimizer.options['w'] = w\n",
        "    optimizer.optimize(objective, iters=1, verbose=False)\n",
        "    cost_history.append(optimizer.swarm.best_cost)\n",
        "    pos_history.append(optimizer.swarm.best_pos.copy())\n",
        "    swarm_history.append(optimizer.swarm.position.copy())\n",
        "    velocity_history.append(optimizer.swarm.velocity.copy())\n",
        "\n",
        "best_cost = optimizer.swarm.best_cost\n",
        "best_pos = optimizer.swarm.best_pos\n",
        "\n",
        "print(\"Optimization is completed!\")\n",
        "print(best_cost, best_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1816890c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "1816890c",
        "outputId": "f4df1a67-87dd-48b1-ff1b-a2f25837be1a"
      },
      "outputs": [],
      "source": [
        "# --- Plotting Section ---\n",
        "# 1st subfigure: contour of ZVS\n",
        "# The rest: current stress contours at selected optimization steps\n",
        "\n",
        "# Prepare grid for contour plots\n",
        "d1_vals = np.linspace(d1_min, d1_max, 100)\n",
        "d2_vals = np.linspace(d2_min, d2_max, 100)\n",
        "D1_grid, D2_grid = np.meshgrid(d1_vals, d2_vals)\n",
        "ZVS_grid = np.zeros_like(D1_grid)\n",
        "CurrentStress_grid = np.zeros_like(D1_grid)\n",
        "\n",
        "# Compute ZVS grid (for 1st subplot)\n",
        "model_NN_classification.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(D1_grid.shape[0]):\n",
        "        for j in range(D1_grid.shape[1]):\n",
        "            X_grid = np.array([[P_fixed, Vref_fixed, D1_grid[i, j], D2_grid[i, j]]])\n",
        "            X_grid_scaled = scaler.transform(X_grid)\n",
        "            X_grid_tensor = torch.FloatTensor(X_grid_scaled)\n",
        "            zvs_output = model_NN_classification(X_grid_tensor)\n",
        "            zvs_pred = torch.argmax(zvs_output, dim=1).item()\n",
        "            zvs_mapping = {0: 4, 1: 6, 2: 8}\n",
        "            ZVS_grid[i, j] = zvs_mapping[zvs_pred]\n",
        "\n",
        "# Compute current stress grid (for later subplots)\n",
        "model_NN_regression.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(D1_grid.shape[0]):\n",
        "        for j in range(D1_grid.shape[1]):\n",
        "            X_grid = np.array([[P_fixed, Vref_fixed, D1_grid[i, j], D2_grid[i, j]]])\n",
        "            X_grid_scaled = scaler.transform(X_grid)\n",
        "            X_grid_tensor = torch.FloatTensor(X_grid_scaled)\n",
        "            current_stress = model_NN_regression(X_grid_tensor).squeeze().item()\n",
        "            CurrentStress_grid[i, j] = current_stress\n",
        "\n",
        "# Choose which iterations to plot (e.g., 1st, 25th, 50th, last)\n",
        "plot_steps = [0, n_iters//4, n_iters//2, n_iters-1]\n",
        "n_plots = 1 + len(plot_steps)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n",
        "\n",
        "# 1st subplot: ZVS contour\n",
        "ax = axes[0]\n",
        "contour = ax.contourf(D1_grid, D2_grid, ZVS_grid, levels=[3,5,7,9], colors=['#fbb4ae','#b3cde3','#ccebc5'])\n",
        "cbar = fig.colorbar(contour, ax=ax, ticks=[4,6,8])\n",
        "ax.set_title('ZVS Region Contour')\n",
        "ax.set_xlabel('D1')\n",
        "ax.set_ylabel('D2')\n",
        "ax.set_xlim([d1_min, d1_max])\n",
        "ax.set_ylim([d2_min, d2_max])\n",
        "\n",
        "# Overlay the best position\n",
        "ax.plot(best_pos[0], best_pos[1], 'r*', markersize=15, label='Best')\n",
        "ax.legend()\n",
        "\n",
        "# Next subplots: current stress contour + swarm positions at selected steps\n",
        "for idx, step in enumerate(plot_steps):\n",
        "    ax = axes[idx+1]\n",
        "    cs_contour = ax.contourf(D1_grid, D2_grid, CurrentStress_grid, levels=20, cmap='viridis', alpha=0.8)\n",
        "    if idx == 0:\n",
        "        # Add colorbar only to the first current stress subplot\n",
        "        fig.colorbar(cs_contour, ax=ax, label='Current Stress')\n",
        "    swarm_pos = swarm_history[step]\n",
        "    ax.scatter(swarm_pos[:,0], swarm_pos[:,1], c='k', s=30, label=f'Swarm (iter {step+1})')\n",
        "    ax.plot(best_pos[0], best_pos[1], 'r*', markersize=15, label='Best')\n",
        "    ax.set_title(f'Current Stress & Swarm at Iter {step+1}')\n",
        "    ax.set_xlabel('D1')\n",
        "    ax.set_ylabel('D2')\n",
        "    ax.set_xlim([d1_min, d1_max])\n",
        "    ax.set_ylim([d2_min, d2_max])\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the best solution\n",
        "X_best = np.array([[P_fixed, Vref_fixed, best_pos[0], best_pos[1]]])\n",
        "X_best_scaled = scaler.transform(X_best)\n",
        "X_best_tensor = torch.FloatTensor(X_best_scaled)\n",
        "\n",
        "model_NN_regression.eval()\n",
        "model_NN_classification.eval()\n",
        "with torch.no_grad():\n",
        "    best_current_stress = model_NN_regression(X_best_tensor).item()\n",
        "    best_zvs_prediction = model_NN_classification(X_best_tensor).argmax(dim=1).item()\n",
        "    zvs_mapping = {0: 4, 1: 6, 2: 8}\n",
        "    best_zvs_prediction = zvs_mapping[best_zvs_prediction]\n",
        "\n",
        "print(f\"Optimized current stress: {best_current_stress:.4f}\")\n",
        "print(f\"Target ZVS devices: {best_zvs_prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80738407",
      "metadata": {
        "id": "80738407"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
